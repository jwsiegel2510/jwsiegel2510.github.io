<!DOCTYPE html>
<html>
<head>

    <title>Jonathan W. Siegel</title>
	
	<meta name="author" content="Jonathan Siegel" />
	<meta name="description" content="My Academic Website" />

	<link rel="stylesheet" href="stylesheet.css" type="text/css" />
	
</head>
<body>


	<div id="page">
		<div id="logo">
			<h1><p id="logoLink">Jonathan Siegel</p></h1>
			<h3>	Texas A&M Mathematics Department <br>
				College Station, TX 77840<h3>
			
		</div>
		<div id="nav">
			<ul>
				<li><a href="index.html">Home</a></li>
				<li><a href="research.html">Research</a></li>
				<li><a href="teaching.html">Teaching</a></li>
				<li><a href="misc.html">Miscellaneous</a></li>
			</ul>	
		</div>
		<div id="content">
			<h2><b>Research</b></h2>
			<p>
				Currently, my research interests are centered around the mathematical theory behind machine learning, and the application of
				machine learning techniques to the solution of PDEs. Important in this line of work are approximation theory,
				specifically the approximation theory of neural networks, functional analysis, statistics, and information theory. 
				Recent projects include the determination of optimal approximation rates for deep ReLU neural networks on Sobolev spaces, 
				the determination of fundamental approximation theoretic quantities, such as the entropy and n-widths, of
				classes of functions corresponding to shallow neural networks, and a theoretical analysis of greedy algorithms for statistical estimation 
				and for numerically solving PDEs.
				
			</p>
			<p> 
				In addition, I have previously worked on, and continue to work on, a variety of other projects. 
				These topics include convex optimization and optimization on 
				manifolds, the application of compressed sensing to electronic structure calculations, signal processing, and neural network
				training, and the application of machine learning to materials science.
			</p>
			<p>     I worked as a postdoc with Professor Jinchao Xu from 2018-2022 and completed my PhD under Professor Russel Caflisch in 2018. Here is my <a href="CV.pdf"><b>CV</b></a>.
			<h2><b> Journal Articles </b></h2>
			<p> <a href="https://www.intlpress.com/site/pub/pages/journals/items/cms/content/vols/0015/0006/a013/index.php?mode=ns"><b> 
			Compact Support Of L1 Penalized Variational Problems </b></a> <br> <i> Communications in Mathematical Sciences (2017) </i> (with Omer Tekin)</p>
			<p> <a href="https://epubs.siam.org/doi/abs/10.1137/18M1220595"><b>
			Accuracy, Efficiency and Optimization of Signal Fragmentation </b></a> <br> <i> Multiscale Simulation and Modelling (2020) </i> (with Russel Caflisch and Edward Chou)</p>			
			<p> <a href="https://arxiv.org/abs/1904.02311"><b>
			Approximation Rates for Neural Networks with General Activation Functions </b></a> <br> <i> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608020301891"> Neural Networks (2020) </a></i> (with Jinchao Xu)</p>			
			<p> <a href="https://arxiv.org/abs/1903.05204"><b> 
			Accelerated Optimization with Orthogonality Constraints </b></a> <br> <i> <a href="http://www.global-sci.org/jcm/">Journal of Computational Mathematics</a> (2020) </i></p>
			<p> <a href="https://arxiv.org/abs/2012.07205"> <b> High-Order Approximation Rates for Shallow Neural Networks with Cosine and ReLUk Activation Functions </b> </a><br> <i> <a href="https://www.journals.elsevier.com/applied-and-computational-harmonic-analysis">Applied and Computational Harmonic Analysis</a> (2022)</i> (with Jinchao Xu) </p>

			<p> <a href="https://arxiv.org/abs/2106.15000"> <b> Optimal Convergence Rates for the Orthogonal Greedy Algorithm </b> </a> <br> <i> IEEE Transactions on Information Theory (2022) </i> (with Jinchao Xu) </p>
			<p> <a href="https://arxiv.org/abs/2008.13654"> <b> Extensible Structure-Informed Prediction of Formation Energy with Improved Accuracy and Usability employing Neural Networks</b> </a> <br> <i> Computational Materials Science (2022) </i> (with Adam Krajewski, Zi-Kui Liu, and Jinchao Xu) </p>
			<p> <a href="https://link.springer.com/article/10.1007/s40687-022-00346-y"> <b> Uniform approximation rates and metric entropy of shallow neural networks </b> </a> <br> <i> Research in the Mathematical Sciences (2022) </i> (with Limin Ma and Jinchao Xu) </p>
			<p> <a href="https://link.springer.com/article/10.1007/s10208-022-09595-3"> <b> Sharp Bounds on the Approximation Rates, Metric Entropy, and n-Widths of Shallow Neural Networks </b> </a> <br> <i> Foundations of Computational Mathematics (2022) </i> (with Jinchao Xu) </p>				
			<p> <a href="https://www.global-sci.org/intro/article_detail/jcm/21637.html"><b> 
			Extended Regularized Dual Averaging Methods for Stochastic Optimization </b></a> <br> <i> Journal of Computational Mathematics (2023) </i> (with Jinchao Xu) </p>	
			<p> <a href="https://link.springer.com/article/10.1007/s00365-023-09626-4"> <b> Characterization of the Variation Spaces Corresponding to Shallow Neural Networks </b> </a> <br> <i> Constructive Approximation (2023) </i>(with Jinchao Xu) </p>	
			<p> <a href="https://www.sciencedirect.com/science/article/pii/S0021999123001791"> <b> Greedy Training Algorithms for Neural Networks and Applications to PDEs </b> </a> <br> <i>Journal of Computational Physics (2023) </i> (with Qingguo Hong, Wenrui Hao, Xianlin Jin and Jinchao Xu) </p>
			<p> <a href="https://jmlr.org/papers/volume24/23-0025/23-0025.pdf"> <b> Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces </b> </a> <br> <i>Journal of Machine Learning Research (2023) </i> </p>
			<h2> <b>Preprints and Works in Progress </b></h2>
			<p> <a href="https://arxiv.org/abs/1903.05671"><b> 
			Accelerated First-Order Methods: Differential Equations and Lyapunov Functions </b></a> </p>
			<p> <a href="https://arxiv.org/abs/2008.09661"><b> Training Sparse Neural Networks using Compressed Sensing </b></a> <i> (with Jianhong Chen and Jinchao Xu) </i> </p>			
			<p> <a href="https://arxiv.org/abs/2302.00834"> <b> Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at Irregularly Spaced Data </b> </a> </p>
			<p> <a href="https://arxiv.org/abs/2302.05515"> <b> Achieving acceleration despite very noisy gradients </b> </a> <i> (with Kanan Gupta and Stephan Wojtowsytsch) </i> </p>
			<p> <a href="https://arxiv.org/abs/2304.13332"> <b> Entropy-based convergence rates of greedy algorithms </b> </a> <i> (with Yuwen Li) </i> </p>
			<p> <a href="https://arxiv.org/abs/2307.07679"> <b> Sharp Convergence Rates for Matching Pursuit </b> </a> <i> (with Jason Klusowski) </i> </p>
			<p> <a href="https://arxiv.org/abs/2307.15285"> <b>Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks </b> </a> <i> </i> </p>
			<p> <a href="https://arxiv.org/abs/2307.15772"> <b>Weighted variation spaces and approximation by shallow ReLU networks </b> </a> <i> (with Ronald DeVore, Robert Nowak, and Rahul Parhi) </i> </p>
	</div>
</body>
</html>
