<!DOCTYPE html>
<html>
<head>

    <title>Jonathan W. Siegel</title>
	
	<meta name="author" content="Jonathan W. Siegel" />
	<meta name="description" content="My Academic Website" />

	<link rel="stylesheet" href="stylesheet.css" type="text/css" />
	
</head>
<body>


	<div id="page">
		<div id="logo">
			<h1><p id="logoLink">Jonathan Siegel</p></h1>
			<h3>	Texas A&M Mathematics Department <br>
				College Station, TX 77840<h3>
			
		</div>
		<div id="nav">
			<ul>
				<li><a href="index.html">Home</a></li>
				<li><a href="research.html">Research</a></li>
				<li><a href="teaching.html">Teaching</a></li>
				<li><a href="misc.html">Miscellaneous</a></li>
			</ul>	
		</div>
		<div id="content">
			<h1><b>Research</b></h1>
			<p>
				My research interests are currently centered around approximation theory and its adjacent fields of
				mathematics and science, including deep learning, statistics, and numerical methods for solving PDEs.
				I am also interested in the application of machine learning to materials science.
			</p>
			<p>     I worked as a postdoc with Professor Jinchao Xu from 2018-2022 and completed my PhD under Professor Russel Caflisch in 2018. Here is my <a href="CV.pdf"><b>CV</b></a>. A list of my publications can also be found on my <a href="https://scholar.google.com/citations?user=oI42qIIAAAAJ"><b>Google Scholar Profile</b></a>.
			<h1><b> Journal Articles </b></h1>
			<p> <a href="https://www.sciencedirect.com/science/article/pii/S0885064X2400061X"> <b> Sharp Lower Bounds on the Manifold Widths of Sobolev and Besov Spaces </b> </a> <br> <i>Journal of Complexity (2024) </i> </p>
			<p> <a href="https://www.worldscientific.com/doi/abs/10.1142/S0218202524500143"> <b> Entropy-based Convergence Rates of Greedy Algorithms </b> </a> <br> <i>Mathematical Models and Methods in Applied Sciences (2024) </i> (with Yuwen Li) </p>
			<p> <a href="https://jmlr.org/papers/volume24/23-0025/23-0025.pdf"> <b> Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces </b> </a> <br> <i>Journal of Machine Learning Research (2023) </i> </p>
			<p> <a href="https://www.sciencedirect.com/science/article/pii/S0021999123001791"> <b> Greedy Training Algorithms for Neural Networks and Applications to PDEs </b> </a> <br> <i>Journal of Computational Physics (2023) </i> (with Qingguo Hong, Wenrui Hao, Xianlin Jin and Jinchao Xu) </p>	
			<p> <a href="https://link.springer.com/article/10.1007/s00365-023-09626-4"> <b> Characterization of the Variation Spaces Corresponding to Shallow Neural Networks </b> </a> <br> <i> Constructive Approximation (2023) </i>(with Jinchao Xu) </p>	
			<p> <a href="https://www.global-sci.org/intro/article_detail/jcm/21637.html"><b> 
			Extended Regularized Dual Averaging Methods for Stochastic Optimization </b></a> <br> <i> Journal of Computational Mathematics (2023) </i> (with Jinchao Xu) </p>	
			<p> <a href="https://link.springer.com/article/10.1007/s10208-022-09595-3"> <b> Sharp Bounds on the Approximation Rates, Metric Entropy, and n-Widths of Shallow Neural Networks </b> </a> <br> <i> Foundations of Computational Mathematics (2022) </i> (with Jinchao Xu) </p>				
			<p> <a href="https://link.springer.com/article/10.1007/s40687-022-00346-y"> <b> Uniform approximation rates and metric entropy of shallow neural networks </b> </a> <br> <i> Research in the Mathematical Sciences (2022) </i> (with Limin Ma and Jinchao Xu) </p>
			<p> <a href="https://arxiv.org/abs/2008.13654"> <b> Extensible Structure-Informed Prediction of Formation Energy with Improved Accuracy and Usability employing Neural Networks</b> </a> <br> <i> Computational Materials Science (2022) </i> (with Adam Krajewski, Zi-Kui Liu, and Jinchao Xu) </p>
			<p> <a href="https://arxiv.org/abs/2106.15000"> <b> Optimal Convergence Rates for the Orthogonal Greedy Algorithm </b> </a> <br> <i> IEEE Transactions on Information Theory (2022) </i> (with Jinchao Xu) </p>
			<p> <a href="https://arxiv.org/abs/2012.07205"> <b> High-Order Approximation Rates for Shallow Neural Networks with Cosine and ReLUk Activation Functions </b> </a><br> <i> <a href="https://www.journals.elsevier.com/applied-and-computational-harmonic-analysis">Applied and Computational Harmonic Analysis</a> (2022)</i> (with Jinchao Xu) </p>
			<p> <a href="https://arxiv.org/abs/1903.05204"><b> 
			Accelerated Optimization with Orthogonality Constraints </b></a> <br> <i> <a href="http://www.global-sci.org/jcm/">Journal of Computational Mathematics</a> (2020) </i></p>
			<p> <a href="https://arxiv.org/abs/1904.02311"><b>
			Approximation Rates for Neural Networks with General Activation Functions </b></a> <br> <i> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608020301891"> Neural Networks (2020) </a></i> (with Jinchao Xu)</p>			
			<p> <a href="https://epubs.siam.org/doi/abs/10.1137/18M1220595"><b>
			Accuracy, Efficiency and Optimization of Signal Fragmentation </b></a> <br> <i> Multiscale Simulation and Modelling (2020) </i> (with Russel Caflisch and Edward Chou)</p>			
			<p> <a href="https://www.intlpress.com/site/pub/pages/journals/items/cms/content/vols/0015/0006/a013/index.php?mode=ns"><b> 
			Compact Support Of L1 Penalized Variational Problems </b></a> <br> <i> Communications in Mathematical Sciences (2017) </i> (with Omer Tekin)</p>

			<h1> <b>Conference Papers </b></h1>
			<p> <a href="https://arxiv.org/pdf/2402.16077v2"> <b> Equivariant Frames and the Impossibility of Continuous Canonicalization </b> </a> <br> <i> International Conference on Machine Learning (2024) </i> (with Nadav Dym and Hannah Lawrence) </p>
			<h1> <b>Preprints and Works in Progress </b></h1>
			<p> <a href="https://arxiv.org/abs/2408.10996"> <b>Approximation Rates for Shallow ReLUk Neural Networks on Sobolev Spaces via the Radon Transform </b> </a> <i> (with Tong Mao and Jinchao Xu) </i> </p>
			<p> <a href="https://arxiv.org/abs/2406.09217"> <b>Convergence and error control of consistent PINNs for elliptic PDEs </b> </a> <i> (with Andrea Bonito, Ronald DeVore, and Guergana Petrova) </i> </p>
			<p> <a href="https://arxiv.org/abs/2404.02849"> <b>Efficient Structure-Informed Featurization and Property Prediction of Ordered, Dilute, and Random Atomic Structures </b> </a> <i> (with Adam Krajewski and Zi-Kui Liu) </i> </p>
			<p> <a href="https://arxiv.org/abs/2307.15772"> <b>Weighted variation spaces and approximation by shallow ReLU networks </b> </a> <i> (with Ronald DeVore, Robert Nowak, and Rahul Parhi) </i> </p>
			<p> <a href="https://arxiv.org/abs/2307.07679"> <b> Sharp Convergence Rates for Matching Pursuit </b> </a> <i> (with Jason Klusowski) </i> </p>
			<p> <a href="https://arxiv.org/abs/2307.15285"> <b>Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks </b> </a> <i> </i> </p>
			<p> <a href="https://arxiv.org/abs/2302.05515"> <b> Achieving acceleration despite very noisy gradients </b> </a> <i> (with Kanan Gupta and Stephan Wojtowsytsch) </i> </p>
			<p> <a href="https://arxiv.org/abs/2302.00834"> <b> Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at Irregularly Spaced Data </b> </a> </p>
			<p> <a href="https://arxiv.org/abs/2008.09661"><b> Training Sparse Neural Networks using Compressed Sensing </b></a> <i> (with Jianhong Chen and Jinchao Xu) </i> </p>			
			<p> <a href="https://arxiv.org/abs/1903.05671"><b> Accelerated First-Order Methods: Differential Equations and Lyapunov Functions </b></a> </p>
	</div>
</body>
</html>
