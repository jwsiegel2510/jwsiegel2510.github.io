<!DOCTYPE html>
<html>
<head>

    <title>Jonathan W. Siegel</title>
	
	<meta name="author" content="Jonathan Siegel" />
	<meta name="description" content="My Academic Website" />

	<link rel="stylesheet" href="stylesheet.css" type="text/css" />
	
</head>
<body>


	<div id="page">
		<div id="logo">
			<h1><p id="logoLink">Jonathan Siegel</p></h1>
			<h3>	Texas A&M Mathematics Department <br>
				College Station, TX 77840<h3>
			
		</div>
		<div id="nav">
			<ul>
				<li><a href="index.html">Home</a></li>
				<li><a href="research.html">Research</a></li>
				<li><a href="teaching.html">Teaching</a></li>
				<li><a href="misc.html">Miscellaneous</a></li>
			</ul>	
		</div>
		<div id="content">
			<h2><b>Research</b></h2>
			<p>
				Currently, my research interests are centered around the mathematical theory behind machine learning, and the application of
				machine learning techniques to the solution of PDEs. Important in this line of work are approximation theory,
				specifically the approximation theory of neural networks, functional analysis, statistics, and information theory. 
				Recent projects include the determination of fundamental approximation theoretic quantities, such as the entropy and n-widths, of
				classes of functions corresponding to shallow neural networks, and a theoretical analysis of greedy algorithms on such function classes.
				
			</p>
			<p> 
				In addition, I have previously worked, and continue to work, on a variety of other projects. 
				These topics include convex optimization and optimization on 
				manifolds, the application of compressed sensing to electronic structure calculations, signal processing, and neural network
				training, and the application of machine learning to materials science.
			</p>
			<p>     I am currently working with Professor Jinchao Xu and did my PhD under Professor Russel Caflisch. Here is my <a href="CV.pdf"><b>CV</b></a>.
			<h2><b> Journal Articles </b></h2>
			<p> <a href="https://www.intlpress.com/site/pub/pages/journals/items/cms/content/vols/0015/0006/a013/index.php?mode=ns"><b> 
			Compact Support Of L1 Penalized Variational Problems </b></a> <br> <i> Communications in Mathematical Sciences (2017) </i> (with Omer Tekin)</p>
			<p> <a href="https://epubs.siam.org/doi/abs/10.1137/18M1220595"><b>
			Accuracy, Efficiency and Optimization of Signal Fragmentation </b></a> <br> <i> Multiscale Simulation and Modelling (2020) </i> (with Russel Caflisch and Edward Chou)</p>			
			<p> <a href="https://arxiv.org/abs/1904.02311"><b>
			Approximation Rates for Neural Networks with General Activation Functions </b></a> <br> <i> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608020301891"> Neural Networks (2020) </a></i> (with Jinchao Xu)</p>			
			<p> <a href="https://arxiv.org/abs/1903.05204"><b> 
			Accelerated Optimization with Orthogonality Constraints </b></a> <br> <i> <a href="http://www.global-sci.org/jcm/">Journal of Computational Mathematics</a> (2020) </i></p>
			<p> <a href="https://arxiv.org/abs/2012.07205"> <b> High-Order Approximation Rates for Shallow Neural Networks with Cosine and ReLUk Activation Functions </b> </a><br> <i> <a href="https://www.journals.elsevier.com/applied-and-computational-harmonic-analysis">Applied and Computational Harmonic Analysis</a> (2022)</i> (with Jinchao Xu) </p>

			<p> <a href="https://arxiv.org/abs/2106.15000"> <b> Optimal Convergence Rates for the Orthogonal Greedy Algorithm </b> </a> <br> <i> IEEE Transactions on Information Theory (2022) </i> (with Jinchao Xu) </p>
			<p> <a href="https://arxiv.org/abs/2008.13654"> <b> Extensible Structure-Informed Prediction of Formation Energy with Improved Accuracy and Usability employing Neural Networks</b> </a> <br> <i> Computational Materials Science (2022) </i> (with Adam Krajewski, Zi-Kui Liu, and Jinchao Xu) </p>
			<p> <a href="https://link.springer.com/article/10.1007/s40687-022-00346-y"> <b> Uniform approximation rates and metric entropy of shallow neural networks </b> </a> <br> <i> Research in the Mathematical Sciences (2022) </i> (with Limin Ma and Jinchao Xu) </p>
						
			<h2> <b>Preprints and Works in Progress </b></h2>
			<p> <a href="https://arxiv.org/abs/1903.05671"><b> 
			Accelerated First-Order Methods: Differential Equations and Lyapunov Functions </b></a> </p>
			<p> <a href="ftp://ftp.math.ucla.edu/pub/camreport/cam17-69.pdf"><b> 
			Shift Invariant Subspaces and Applications to Signal Fragmentation </b></a> </p>
			<p> <a href="https://arxiv.org/abs/1904.02316"><b> 
			The Extended Regularized Dual Averaging Method for Composite Optimization </b></a> <i> (with Jinchao Xu) </i> </p>
			<p> <a href="https://arxiv.org/abs/2008.09661"><b> Training Sparse Neural Networks using Compressed Sensing </b></a> <i> (with Jianhong Chen and Jinchao Xu) </i> </p>			
			<p> <a href="https://arxiv.org/abs/2101.12365"><b> Sharp Bounds on the Approximation Rates, Metric Entropy, and n-widths of Shallow Neural Networks </b></a> <i> (with Jinchao Xu) </i> </p>
			<p> <a href="https://arxiv.org/abs/2106.15002"> <b> Characterization of the Variation Spaces Corresponding to Shallow Neural Networks </b> </a> <i> (with Jinchao Xu) </i> </p>
			<p> <a href="https://arxiv.org/abs/2107.04466"> <b> Greedy Training Algorithms for Neural Networks and Applications to PDEs </b> </a> <i> (with Qingguo Hong, Wenrui Hao, Xianlin Jin and Jinchao Xu) </i> </p>
		</div>
</body>
</html>
